{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Data Journalism\n",
    "## HODP Bootcamp Week 5\n",
    "### October 15, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to make sure you have the requirements \n",
    "# It should return 0. Then, comment it out. \n",
    "\n",
    "import os \n",
    "result = os.system(\"python3 -m pip install -r requirements.txt\")\n",
    "if result != 0:\n",
    "    os.system(\"python -m pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals for this week:\n",
    "* Significance tests\n",
    "  - Two sample t-tests\n",
    "  - One sample t-tests\n",
    "  - Permutation tests\n",
    "* Confidence intervals\n",
    "* Understand the difference between correlation and causation\n",
    "* Practice basic linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data\n",
    "Here is a dataset of US House election data in 2018. I've taken the liberty of cleaning the data for you using R. The original dataset is from MIT election labs and is also included as is my cleaning code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "house = pd.read_csv(\"house_elections_2018_clean.csv\")\n",
    "house.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Tests and P-Values\n",
    "### Did Rebublicans and Democrats win significantly different proportions of the vote in 2018 House elections? \n",
    "#### But first, a quick note on Parameters vs. Statistics\n",
    "* In data journalism, we're often interested in estimating *pararameters* of interest, which are fixed but often unknown quantities.\n",
    "* We estimate these with *sample statistics*, which are known instantiates of random estimators that vary as a function of our data. \n",
    "* Mixing up the two is a common pitfall in journalism.\n",
    "\n",
    "#### Two sample t-tests\n",
    "We're going to conduct a two sample t-test. We can do this with SciPy's built in t-test function. A two sample t-test is a test of how \"far\" two sample statistics are from each other; we're usually trying to see if they are significantly different from each other. Let's use Python to see if the proportions of votes for Dems and Repubs are significantly different.\n",
    "\n",
    "This testing paradigm assumes that our observations are normally distributed and independent of each other. Assuming our sample design is good, we can assume our observations are independent. Let's plot a histogram of our observations to make sure the assumption of normality is satisfied using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "democrats = house[house.party == 'democrat']\n",
    "republicans = house[house.party == 'republican']\n",
    "n_bins = 20\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "ax[0].hist(democrats.prop, bins=n_bins)\n",
    "ax[0].set_title(\"Democrats Prop\", fontsize=14)\n",
    "ax[1].hist(republicans.prop, bins=n_bins)\n",
    "ax[1].set_title(\"Republican Prop\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've checked our assumption of normality, we can perform our t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "print(\"Democratic minus Republican proportion:\", np.mean(democrats.prop) - np.mean(republicans.prop))\n",
    "stats.ttest_ind(democrats.prop, republicans.prop, equal_var = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "Now, we have a p-value. The p-value is often misunderstood or improperly interpreted, so it's very important to know what it actually means. **A p-value is the probability of observing our data, given that the null hypothesis is correct.** So, we assume that our data are random, but the true values are fixed. A common pitfall when interpreting p-values is to think of the p-value as the probability of the null hypothesis being correct, but this interpretation is wrong. Looking at the results above with this in mind, we can see that if Democrats and Republicans truly had the same average ranking, there would be very low probability of getting the data we got (though still possible), so we can pretty reasonably reject the null hypothesis and say that the two parties do not have equal proportions. Because the test statistic is negative, we can see that Adams has a lower (better) ranking. \n",
    "\n",
    "In general, a common rule of thumb is to reject the null hypothesis when the p-value is less than 0.05. When we fail to reject the null hypothesis, we should never say that we \"accept\" or \"prove\" the null. All we have done is failed to reject it; we can never prove the null hypothesis true.\n",
    "\n",
    "In this case, with a pvalue < 0.001, we can reject the null hypothesis. We have significant evidence that Republicans and Democrats had different vote proportions on average in 2018 house elections and that, on average, Democrats had higher proportions than Republicans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sample tests\n",
    "\n",
    "We can also do one-sample tests, where we compare our statistic to a set value. A one sample t-test is a test of how \"far\" a sample statistic is from a hypothesized \"true\" value, still assuming normality of observations. For example, if Dean Khurana tells you that he believes 75% of students support the social group sanctions, but your HODP survey suggests that only 65% of students support the sanctions, you can test whether that difference is sufficiently large to dispute Khurana's claim.\n",
    "\n",
    "To run such a test, you can use the stats.ttest_1samp() command (documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html)\n",
    "\n",
    "**Let's say Fox News claims that the true proportion of American that voted for Republicans in 2018 was 49%. Perform a one sample t-test to validate this claim. Comment on your assumptions for a one sample t-test and interpret your pvalue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Interpretation and comment on Assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Tests\n",
    "If our normality assumptions fail for our observations, then we shouldn't use t-tests. In that case, we would be better off performing a nonparametric test like a permutation test. A permutation works by assuming that the labels of democrat and republican make no difference in determining proportions. Therefore, shuffling up the labels should not affect averages between the two groups. \n",
    "\n",
    "The test works by shuffling labels and computing differences in means many times. This produces a distribution of test statistics. Then, we calculate a p-value by finding the probability of observing a test statistic as extreme or more extreme than the one produced by our particular permutation of labels and proportions. \n",
    "\n",
    "Code to do this is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import permutation_test\n",
    "import seaborn as sns\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(10, 5))\n",
    "sns.boxplot(x='party', y='prop', data=house)\n",
    "ax.set_xlabel(\"Party\", fontsize=14)\n",
    "ax.set_ylabel(\"Proportion\", fontsize=14)\n",
    "ax.set_title(\"Party vote proportions\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "p_value = permutation_test(democrats.prop, republicans.prop,\n",
    "                           method='approximate',\n",
    "                           num_rounds=10000,\n",
    "                           seed=0)\n",
    "\n",
    "p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test does not assume normality, but it does assume that the two groups are drawn from the same distribution. This assumption needs to be checked as well. We will check it by using boxplots. The trade off is that while permutation tests are great for calculating p-values, unlike t-tests they aren't great for confidence intervals. When you conduct a t-test, you basically get a confidence interval for free since you know the distribution of your test statistic. \n",
    "\n",
    "BUT SETH! What's a confidence interval? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals\n",
    "A confidence interval is a range of plausible values for your parameter. You're probably quite familiar with point estimates in Statistics, where we say the observed proportion of democratic votes is, say, 0.53. Your estimator here is a random variable, a function of your data, that estimates a parameter - the true proportion of democratic votes. A confidence interval is, as the name suggests, an interval that we think the true value is likely to fall in. These intervals have a certain level of *confidence*, or the percentage of intervals calculated using this method that contain the true value. There is a trade off between confidence and width - higher confidence is great, but it will also widen the interval, which can make it less useful. For example, you can construct a 100% confidence interval for anything, but it goes from negative infinity to positive infinity, and tells us nothing about the parameter of interest.\n",
    "\n",
    "In practice, 95% confidence intervals are quite popular, which means that, on average, 19 out of every 20 contain the true value of the parameter of interest. A common pitfall with confidence intervals is to say there is a 95% chance that the true value falls inside the interval, but this interpretation is incorrect, so please stay away from this phrase in your articles.\n",
    "\n",
    "Now, let's construct a 95% confidence interval for the proportion of democratic votes. We have to do a bit more work to generate a confidence interval in Python. We need to provide SciPy's interval function with a confidence level, a center (the sample mean), and a scale (the standard error, which is the sample standard deviation divided by the square root of one less than the length of the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, sigma = np.mean(democrats.prop), np.std(democrats.prop)\n",
    "\n",
    "conf_int = stats.norm.interval(0.95, \n",
    "                               loc = mean, \n",
    "                               scale = sigma/np.sqrt(len(democrats.prop) - 1))\n",
    "conf_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try generating an interval for the republican data using a different confidence level, and interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the interval here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret your results here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation vs. Correlation\n",
    "You've all probably heard this before, but it's important to hear it again: correlation does not imply causation. It's pretty rare that we'll be able to show causation in a HODP article, so it's important to frame most of our work as a correlation or trend we noticed, rather than as a direct cause. Often, though, it will intuitively make sense that there \"should\" or at least \"could\" be a causal connection. In those cases, make sure to frame your writing as a \"possible explanation\" than as a statement of what is going on. For example, the percentage of female concentrators by department is likely strongly correlated with the percentage of female faculty members, and there is probably some causal effect here. However, it's best to cite other research on whether such a trend has a causal effect, or to cite relevant quantitative work. For example, in an article about gender balance in different departments, we could talk about existing research on the effect of faculty gender on students and potentially cite relevant Crimson articles, but we should not conclude that (for example) low female faculty presence in Mathematics *causes* low female student presence in Mathematics.\n",
    "\n",
    "For people who are particularly interested in causation, talk to Seth or look at Stat 111 (Statistical Inference), Stat 186 (Causal Inference), and Ec 1123 (Econometrics).\n",
    "\n",
    "Let's look at some examples of how we might be able to find correlations that are likely not causal. This will also show you how to find a correlation coefficient. If all you want is the correlation, it's very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "rankings = pd.read_csv(\"house_rankings_2018.csv\")\n",
    "houses = rankings.House\n",
    "rankings.set_index(\"House\", inplace = True)\n",
    "rankings\n",
    "#monthly high temps in Boston\n",
    "bostontemps = [37, 39, 46, 57, 67, 77, 82, 81, 73, 62, 52, 42]\n",
    "levCounts = list(rankings.values[5,])\n",
    "pearsonr(levCounts, bostontemps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So though Boston temperature and Leverett ratings are correlated, it's ridiculous to say one causes the other. Correlation, independence, and causation are just all totally different things, so we need to be careful not to mix them up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Regression\n",
    "Regression is a very useful tool for prediction. Linear regressions allow us to easily model a linear relationship between a response/dependent/Y variable and 1 or more predictor/independent/X variables. This is a very widely used technique, so if you plan to use regression in your project, please come talk to us for a more in depth treatment of the subject, but here are the basics! Regressions in Python are fairly easy to do: we just need a Y list, and at least one X list of equal length! Below, we've built a regression to predict first place rankings from distance away from Widener Library. Note that you may sometimes need to reshape data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "#walking time from Widener Library (in minutes), from Google Maps\n",
    "dist = [2, 15, 7, 8, 3, 5, 7, 16, 7, 2, 17, 6]\n",
    "\n",
    "# Reshape distances as a column vector\n",
    "dist_reshaped = np.array(dist).reshape(-1,1)\n",
    "first_place = [rankings.values[i,0] for i in range(0, 12)]\n",
    "\n",
    "#X, Y is the order\n",
    "reg = lm.fit(dist_reshaped, first_place)\n",
    "beta0, beta1 = reg.intercept_, reg.coef_\n",
    "y_predict = np.multiply(dist_reshaped,beta1) + beta0\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "ax.scatter(dist_reshaped, first_place)\n",
    "plt.plot(dist_reshaped, y_predict, '-o',color='r')\n",
    "ax.set_title(\"Predicting First place from Distance from Widener\", fontsize=14)\n",
    "plt.show()\n",
    "print(beta0, beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do this a slightly different way using an OLS object to get more information. You just have to further reformat your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "dist_reshaped_ols = sm.add_constant(dist_reshaped)\n",
    "mod = sm.OLS(first_place,dist_reshaped_ols)\n",
    "results = mod.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting your Coefficients\n",
    "In simple regression, we can interpret the coefficients easily. In this case, the coefficient 1.468 is the change in Leverett ratings for a one unit change in Boston temperature. \n",
    "\n",
    "The coefficient -42.97 is the predicted number of Leverett ratings if the monthly high temperature in Boston was 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Demystifying Beta_1 \n",
    "Somewhat unsurprisingly, there is a relationship between beta_1 and your correlation. It turns out, if your X's and Y's are standardized, then your correlation coefficient is equal to your beta_1 coefficient. You can actually get from beta1 to the correlation coefficient really easily by doing some simple algebra. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demystifying Beta1\n",
    "covariance = beta1 * np.var(dist) \n",
    "cor = covariance / (np.std(dist) * np.std(first_place))\n",
    "print(cor)\n",
    "print(pearsonr(first_place, dist)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "* Linearity\n",
    "* Equal Spread \n",
    "* Normality of Errors\n",
    "* Independence of assumptions\n",
    "\n",
    "We can check the first three of these with a plot of our linear model and a plot of the residuals versus the predicted value. These plots are given below. We can see from the plots that there's a problem. Well two, actually. There are clearly two outliers throwing everything off!\n",
    "\n",
    "Into the weeds about the plots: From the first plot, we see that there appear to be two outliers - influential points - that are skewing our model. The residual histogram seems to make our normality of errors assumption questionable, but that's mostly caused by the outliers as well. Our residuals vs. predicted plot looks terrible as well because of these two outliers. Although there's no obvious pattern in our residuals, they don't appear to have equal spread. \n",
    "\n",
    "Take a Stats course for more clarity on the assumptions. If you don't get all of this, that's okay. You'll never put this in your article, but I would feel like I'm committing statistical malpractice if I didn't at least make you aware of the assumptions regarding regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "fig, ax = plt.subplots(1, 2, tight_layout=True)\n",
    "ax[0].scatter(dist_reshaped, first_place)\n",
    "ax[0].plot(dist_reshaped, y_predict, '-o',color='r')\n",
    "ax[0].set_title(\"Predicting First Place Votes from Distance\", fontsize=14)\n",
    "ax[1].hist(results.resid)\n",
    "ax[1].set_title(\"Residual Histogram\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "visualizer = ResidualsPlot(reg)\n",
    "visualizer.fit(dist_reshaped, first_place)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's investigate those outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings['dist'] = dist \n",
    "rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AHA! The outliers are Lowell and Winthrop! I suspect that the recent renovations are causing these two houses to have lots of first place votes despite their distances from Widener. If only there was some way to control for renovations..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression\n",
    "\n",
    "We can also run a regression model with more than one predictor variables. All you have to do is add the predictors to your design matrix X and use the lm.fit() command. This allows us to control for confounding variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#walking time from Widener Library (in minutes), from Google Maps\n",
    "dist = [2, 15, 7, 8, 3, 5, 7, 16, 7, 2, 17, 6]\n",
    "#was the house renovated in last 4 years? 1 if true\n",
    "renovated = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
    "\n",
    "X = np.matrix([dist, renovated]).transpose()\n",
    "Y = [rankings.values[i,0] for i in range(0, 12)]\n",
    "Y = np.array([Y]).reshape(-1, 1)\n",
    "\n",
    "reg = lm.fit(X, Y)\n",
    "[reg.intercept_, reg.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can do this using an OLS Object as well with properly formatted data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ols = sm.add_constant(X)\n",
    "mod = sm.OLS(Y,X_ols)\n",
    "results = mod.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our prediction lines again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, Y is the order\n",
    "coef = results.params\n",
    "print(coef)\n",
    "y_predict = np.multiply(dist_reshaped,beta1) + beta0\n",
    "\n",
    "x = np.linspace(1,17,100)\n",
    "y_0 = coef[1]*dist_reshaped+coef[0]\n",
    "y_1 = (coef[1])*dist_reshaped + coef[0] + coef[2]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, tight_layout=True)\n",
    "ax.scatter(dist_reshaped, first_place)\n",
    "ax.plot(dist_reshaped, y_0, '-',color='r')\n",
    "ax.plot(dist_reshaped, y_1, '-',color='g')\n",
    "ax.set_title(\"Predicting First Place Votes from Distance\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this helped a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting your Coefficients\n",
    "In multiple regression, we control for certain variables by introducing them as predictors. In this case, the coefficient -1.4065 is the change in first-place votes for a one unit change in distance from Widener library controlling for whether or not a house has been renovated. \n",
    "\n",
    "The coefficient 56.9746 is the change in first-place votes for a one unit change in the renovations indicator variable holding distance from Widener library constant. In other words, if a house is renovated, we can expect a 56.9746 increase in first-place votes, controlling for distance to Widener. \n",
    "\n",
    "Sometimes, we have coefficients whose interpretations don't really make that much sense or that aren't really useful. For example, the coefficient 31.8952 is the expected number of first place votes that a non-renovated house 0 minutes from Widener would receive. While this isn't hard to interpret, it's not that useful unless Harvard was thinking about bulldozing Widener to build a new house. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Finally, one of the most useful things we can do with a predictive model is make predictions! Assuming you called your model `reg`, use the command below to predict the number of first choice votes for Adams House after the renovations begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.predict(np.array([[2, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "I've loaded in a dataset on Presidential elections by state courtesy of Stat 139. Try to predict gap16repub using multiple predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = pd.read_csv(\"pres_elections_bystate.csv\")\n",
    "pres.head()\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "## TODO ##\n",
    "X = np.matrix([pres.gap12repub, pres.governor_repub, pres.hispanic13]).transpose()\n",
    "Y = pres.gap16repub\n",
    "Y = np.array([Y]).reshape(-1, 1)\n",
    "X_ = sm.add_constant(X)\n",
    "\n",
    "reg = lm.fit(X, Y)\n",
    "[reg.intercept_, reg.coef_]\n",
    "\n",
    "mod = sm.OLS(Y,X)\n",
    "results = mod.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
